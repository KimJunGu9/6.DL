{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNf2LrGRWN9qVbzG+nub/k8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KimJunGu9/6.DL/blob/main/11_%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%98_%EC%9D%B4%ED%95%B4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 신경망"
      ],
      "metadata": {
        "id": "t3JUArmmU-m3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1. 인공지능, 머신러닝, 딥러닝의 포함 관계\n",
        "\n",
        "<img src='https://tensorflowkorea.files.wordpress.com/2018/12/028.jpg'>\n",
        "\n",
        "- AI > 머신러닝(SVM, 선형회귀, 로지스틱 회귀, 퍼셉트론..), 규칙기반 > 신경망(DNN, RNN, CNN, GAN ..)\n",
        "- 신경망은 머신러닝 알고리즘 중 하나"
      ],
      "metadata": {
        "id": "Vf9Sd1jvVAaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-2. 신경망의 특징\n",
        "- 인간의 뇌기능을 흉내 내려고 만들어짐\n",
        "- 입력층, 은닉층, 출력층으로 나누어져 있음\n",
        "- 유닛들이 서로 연결된 것이고 그 유닛들 사이의 가중치를 학습하기 위해 사용\n",
        "- 층을 점점 늘려서 깊게 만든 신경망을 심층 신경망(Neural Network)이라고 부름\n",
        "- 깊은 층을 가진 신경망의 가중치를 학습시키는 것을 딥러닝 또는 심층학습이라고 함"
      ],
      "metadata": {
        "id": "IrMSBTyaVCDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-3. 신경망으로 할 수 있는 것\n",
        "* 회귀\n",
        "* 분류\n",
        "* 이미지 생성"
      ],
      "metadata": {
        "id": "Uzs_iCb_VEbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 신경망의 진화"
      ],
      "metadata": {
        "id": "9xxjXYm1VFjM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 1950년대 아이디어(퍼셉트론)가 나옴\n",
        "* 1980년대 오차역전파법으로 신경망 학습을 개발 -> 학습 데이터 부족으로 문제가 발생, 기울기 소실\n",
        "* 2000년대 인터넷이 보급된 후 많은 데이터를 얻을 수 있게 되면서 다시 주목받게 됨\n",
        "* 2012년\n",
        "  * CNN중 Alex net라는 네트워크 구조\n",
        "  * Alex net이 이미지 분류 대회에서 1등을 차지 -> CNN이 유명해지게 됨\n",
        "* 2013년\n",
        "  * 아타리 게임 중 '벽돌깨기' -> AI 작용 -> 학습을 많이 진행 -> 구석을 파서 공을 위로 올리는 방법을 깨달음\n",
        "  * 딥마인드 -> 구글에 인수 -> 알파고\n",
        "* 2014년\n",
        "  * RNN이라는 네트워크를 사용하여 중국어->영어로 번역\n",
        "  * 성능의 한계가 생김\n",
        "  * attention 모델의 출현으로 성능이 급격히 종아짐\n",
        "* 2015년\n",
        "  * GANs -> 오바마 합성사진\n",
        "  * ResNet(Residual Networks): CNN의 종류 중 하나, 사람과 AI가 이미지를 찾는 테스트를 해서, 사람은 5%, ResNet은 3%의 오차률\n",
        "* 2016년\n",
        "  * 알파고를 통해 이세돌9단을 이김\n",
        "  * 이세돌 9단은 컴퓨터를 이긴 마지막 인류\n",
        "* 2017년\n",
        "  * RNN의 단점을 극복한 attention만으로 만든 모델 -> Transformer\n",
        "  * Transformer: 번역 모델\n",
        "  * 자연어를 정복\n",
        "  * Transformer는 딥러닝에서 가장 중요한 모델 중 하나\n",
        "* 2018년\n",
        "  * Transformer에서 인코더만 따온 모델 -> BERT\n",
        "  * 자연어를 정답없이 사용(인터넷 문장들의 데이터를 삽입)\n",
        "  * 일부 글자를 가리고 가린 부분을 맞을 수 있도록 학습\n",
        "  * 모델을 크게 만들고 엄청난 데이터를 사용해야 함\n",
        "* 2019/2020\n",
        "  * GPU 수천대를 사용하여 큰 모델을 만듬\n",
        "  * 정답이 필요없이 실제 데이터만으로 셀프 수퍼바이즈러닝"
      ],
      "metadata": {
        "id": "G3UX58_-VGns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 딥러닝의 키포인트"
      ],
      "metadata": {
        "id": "Fo-qXOyXVI-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 데이터(양질, 많은 양)\n",
        "* 모델(CNN, RNN, Transformer, multilayer perceptron..)\n",
        "* 알고리즘(Gradient Descent를 기초로 많은 알고리즘이 만들어짐)\n",
        "* Loss Function"
      ],
      "metadata": {
        "id": "dUPMwaAeVKM6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iko40ToLVLwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. NN(Neural Network)에서의 핵심"
      ],
      "metadata": {
        "id": "ifz72TQeVMlK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Deep Neural Network: layer(은닉)수가 최소 2개 이상인 network\n",
        "* Loss Function, Cost Function: Neural Network가 얼마나 성능이 좋은지 또는 나쁜지에 대한 척도\n",
        "* loss값을 줄이는 방법 -> 적절한 weight를 찾아야 함"
      ],
      "metadata": {
        "id": "xcgC0yZMVNKj"
      }
    }
  ]
}